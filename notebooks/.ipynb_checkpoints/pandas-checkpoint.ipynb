{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# git2net analysis for the pandas repository\n",
    "\n",
    "First we clone the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygit2 as git2\n",
    "import os\n",
    "import shutil\n",
    "import git2net\n",
    "import pathpy as pp\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import math\n",
    "import copy\n",
    "\n",
    "git_repo_url = 'https://github.com/pandas-dev/pandas.git'\n",
    "#git_repo_url = 'https://github.com/networkx/networkx.git'\n",
    "#git_repo_url = 'https://github.com/numpy/numpy.git'\n",
    "#git_repo_url = 'https://github.com/deepcharles/ruptures.git'\n",
    "#git_repo_url = 'https://github.com/mwaskom/seaborn.git'\n",
    "repo_name = 'pandas'\n",
    "local_directory = '.'\n",
    "git_repo_dir = 'repos/{r}4analysis'.format(r=repo_name)\n",
    "sqlite_db_file = 'databases/{r}/{r}_rename.db'.format(r=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(git_repo_dir):\n",
    "    shutil.rmtree(git_repo_dir)\n",
    "\n",
    "repo = git2.clone_repository(git_repo_url, git_repo_dir) # Clones a non-bare repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the database for it. We will try with max_modification=100, so that most commits are processed.\n",
    "\n",
    "Mine repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove database if exists\n",
    "#if os.path.exists(sqlite_db_file):\n",
    "#    os.remove(sqlite_db_file)\n",
    "\n",
    "max_modifications = 100\n",
    "    \n",
    "git2net.mine_git_repo(git_repo_dir, sqlite_db_file, max_modifications=max_modifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the commits that had more than 100 files modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git2net.mining_state_summary(git_repo_dir, sqlite_db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database modification\n",
    "Replacing the names with number for compatibility with a temporal network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(sqlite_db_file)\n",
    "\n",
    "# Query the db\n",
    "data = pd.read_sql(\"\"\"SELECT * FROM commits\"\"\", con)\n",
    "\n",
    "rename_auth = {}\n",
    "rename_committer = {}\n",
    "\n",
    "i = 1\n",
    "for name in data['author_name'].unique():\n",
    "    rename_auth[name] = i\n",
    "    i = i + 1\n",
    "\n",
    "i = 1\n",
    "for name in data['committer_name'].unique():\n",
    "    rename_committer[name] = i\n",
    "    i = i + 1\n",
    "\n",
    "data = data.replace({'author_name': rename_auth})\n",
    "data = data.replace({'committer_name': rename_committer})\n",
    "\n",
    "data.to_sql('commits', con, if_exists='replace')\n",
    "\n",
    "f = open('databases/{r}/{r}_authors.json'.format(r=repo_name), 'w')\n",
    "f.write(json.dumps(rename_auth))\n",
    "f.close()\n",
    "\n",
    "f = open('databases/{r}/{r}_committers.json'.format(r=repo_name), 'w')\n",
    "f.write(json.dumps(rename_committer))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-author networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is too complex because we consider the whole timeframe of the repository since its creation. Therefore we need to filter the time dimensin, and in order to do that we nee to find the first and last commit in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(sqlite_db_file)\n",
    "max_date = datetime.strptime(pd.read_sql_query(\"SELECT max(committer_date) as max FROM commits\", db)['max'].item(), '%Y-%m-%d %H:%M:%S')\n",
    "min_date = datetime.strptime(pd.read_sql_query(\"SELECT min(committer_date) as min FROM commits\", db)['min'].item(), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print('Min date: ', min_date)\n",
    "print('Max date: ', max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order not to select an 'empty' time period (relatively few commits, e.g. holiday season), it's also worth observing the number of commits over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdCommits = pd.read_sql_query(\"SELECT * FROM commits\", db)\n",
    "\n",
    "days = {(min_date+timedelta(days=x)).date() : 0 for x in range((max_date-min_date).days + 1)}\n",
    "\n",
    "commit_dates = pdCommits['committer_date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').date()).value_counts()\n",
    "\n",
    "for key in commit_dates.keys():\n",
    "    days[key] = commit_dates.get(key)\n",
    "\n",
    "keys = days.keys()\n",
    "values = days.values()\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(keys, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipartite networks\n",
    "Because our network changes over time, we would like to visualize each year consecutively one after the other. We can use the pathpy temporal networks for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collab_network(sqlite_db_file, t, node_info, min_date, max_date, file_base):\n",
    "    if file_base:\n",
    "        start = int(datetime.timestamp(min_date))\n",
    "        end = int(datetime.timestamp(max_date))\n",
    "\n",
    "        n = pp.Network.from_temporal_network(t, min_time=start, max_time=end)\n",
    "\n",
    "        new_n = copy.deepcopy(n)\n",
    "\n",
    "        for node in n.nodes:\n",
    "            if node_info['class'][node] == 'file':\n",
    "                new_n.remove_node(node)\n",
    "\n",
    "        for node in new_n.nodes:\n",
    "            for f in n.successors[node]:\n",
    "                for pre in n.predecessors[f]:\n",
    "                    if not node == pre:\n",
    "                        new_n.add_edge(node, pre)\n",
    "\n",
    "        return new_n\n",
    "    else:\n",
    "        n, node_info, edge_info = git2net.get_coauthorship_network(sqlite_db_file, time_from=min_date, time_to=max_date)\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, node_info, edge_info = git2net.get_bipartite_network(sqlite_db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_from = datetime(2014, 1, 1)\n",
    "time_to = datetime(2014, 12, 31)\n",
    "interval = timedelta(days=30)\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "t2 = pp.TemporalNetwork()\n",
    "\n",
    "rounds = math.ceil((time_to - time_from - interval)/delta)\n",
    "for i in range(0,rounds):\n",
    "    print('Round {i}/{r}'.format(i=i+1, r=rounds+1), end = \"\\r\")\n",
    "    start = time_from + i * delta\n",
    "    end = time_from + i * delta + interval\n",
    "    network = collab_network(sqlite_db_file, t, node_info, start, end, True)\n",
    "\n",
    "    for edge in network.edges:\n",
    "        t2.add_edge(edge[0], edge[1], i)\n",
    "\n",
    "style = {    \n",
    "  'ts_per_frame': 1, \n",
    "  'ms_per_frame': 700,\n",
    "  'look_ahead': 0, \n",
    "  'look_behind': 1, \n",
    "  'inactive_edge_width': 2,\n",
    "  'active_edge_width': 4, \n",
    "  'label_color' : '#000000',\n",
    "  'label_size' : '8px',\n",
    "  'label_offset': [0,5]\n",
    "  }\n",
    "print(t2)\n",
    "#pp.visualisation.plot(t2, **style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_networks(t, node_info, sqlite_db_file, time_from, time_to, interval, delta, file_mode, repo_name):\n",
    "    if len(time_from) == len(time_to) == len(interval) == len(delta) == len(file_mode):\n",
    "        for i in range(0,len(delta)-1):\n",
    "            print('\\nIteration {i}/{r}'.format(i=i+1, r=len(delta)))\n",
    "            t2 = pp.TemporalNetwork()\n",
    "            current_interval = timedelta(days=interval[i])\n",
    "            current_delta = timedelta(days=delta[i])\n",
    "            rounds = math.ceil((time_to[i] - time_from[i] - current_interval)/current_delta)\n",
    "            for j in range(0,rounds):\n",
    "                print('Round {j}/{r}'.format(j=j+1, r=rounds), end = \"\\r\")\n",
    "                start = time_from[i] + j * current_delta\n",
    "                end = time_from[i] + j * current_delta + current_interval\n",
    "                network = collab_network(sqlite_db_file, t, node_info, start, end, file_mode[i])\n",
    "\n",
    "                for edge in network.edges:\n",
    "                    t2.add_edge(edge[0], edge[1], j)\n",
    "\n",
    "            style = {    \n",
    "              'ts_per_frame': 1, \n",
    "              'ms_per_frame': 700,\n",
    "              'look_ahead': 0, \n",
    "              'look_behind': 1, \n",
    "              'inactive_edge_width': 2,\n",
    "              'active_edge_width': 4, \n",
    "              'label_color' : '#000000',\n",
    "              'label_size' : '8px',\n",
    "              'label_offset': [0,5]\n",
    "              }\n",
    "            pp.visualisation.export_html(t2, 'exports/{r}/{r}_tf{tf}_tt{tt}_i{i}_d{d}_{f}.html'.format(\n",
    "                r=repo_name,\n",
    "                tf=time_from[i].strftime('%Y%m%d'),\n",
    "                tt=time_to[i].strftime('%Y%m%d'),\n",
    "                i=interval[i],\n",
    "                d=delta[i],\n",
    "                f='f' if file_mode[i] else 'l'\n",
    "            ), **style)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    print('\\nFinished exporting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, node_info, edge_info = git2net.get_bipartite_network(sqlite_db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/7\n",
      "Iteration 2/7\n",
      "Iteration 3/7\n",
      "Iteration 4/7\n",
      "Iteration 5/7\n",
      "Iteration 6/7\n",
      "Round 50/51\r"
     ]
    }
   ],
   "source": [
    "time_from = [datetime(2014, 1,  1), datetime(2014, 1, 1), datetime(2014, 1,  1), datetime(2014, 1, 1), datetime(2014,  1,  1), datetime(2014,  1,  1), datetime(2014,  1,  1)]\n",
    "time_to   = [datetime(2014, 1, 31), datetime(2014, 2, 6), datetime(2014, 2, 13), datetime(2014, 1, 1), datetime(2014, 12, 31), datetime(2015, 12, 31), datetime(2016, 12, 31)]\n",
    "interval  = [1, 7, 14, 30, 60, 30, 60]\n",
    "delta     = [1, 1,  1,  7,  7, 14, 14]\n",
    "file_mode = [True, True, True, True, True, True, True]\n",
    "\n",
    "export_networks(t, node_info, sqlite_db_file, time_from, time_to, interval, delta, file_mode, repo_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
